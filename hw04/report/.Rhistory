write.csv(stringr_clean, '../data/stringr_archive.csv')
dplyr_raw <- read_archive('dplyr')
dplyr_clean <- clean_archive(dplyr_raw)
write.csv(dplyr_clean, '../data/dplyr_archive.csv')
ggplot2_raw <- read_archive('ggplot2')
ggplot2_clean <- clean_archive(ggplot2_raw)
write.csv(ggplot2_clean, '../data/ggplot2_archive.csv')
XML_raw <- read_archive('XML')
XML_clean <- clean_archive(XML_raw)
write.csv(XML_clean, '../data/XML_archive.csv')
knitr_raw <- read_archive('knitr')
knitr_clean <- clean_archive(knitr_raw)
write.csv(knitr_clean, '../data/knitr_archive.csv')
dplyr_dat <- time_in_yr(dplyr_clean)
ggplot2_dat <- time_in_yr(ggplot2_clean)
XML_dat <- time_in_yr(XML_clean)
knitr_dat <- time_in_yr(knitr_clean)
ggplot() + geom_step(data=dplyr_dat, aes(x=time, y=size, colour='pink')) + geom_point() + geom_step(data=ggplot2_dat, aes(x=time, y=size), colour='green') + geom_point() + geom_step(data=XML_dat, aes(x=time, y=size), colour='purple') + geom_point() + geom_step(data=knitr_dat, aes(x=time, y=size), colour='blue') + geom_point() + xlab('date') + ylab('size(Kilobytes)')
dat <- dplyr_dat %>%
merge(ggplot2_dat,all=TRUE) %>%
merge(XML_dat,all=TRUE) %>%
merge(knitr_dat,all=TRUE)
ggplot(data = dat, aes(x = time, y = size, color = name)) + geom_step() + facet_wrap(~name, scales = 'free')
split_chars('Go Bear!')
split_chars('Expecto Patronum')
vec <- split_chars('Go Bear!')
num_vowels(vec)
count_vowels('The quick brown fox jumps over the lazy dog')
count_vowels('THE QUICK BROWN FOR JUMPS OVER THE LAZY DOG')
reverse_chars('gattaca')
reverse_chars('Lumox Maxima')
reverse_words('sentence! this reverse')
reverse_words('string')
dat <- read.csv('../data/text-emotion.csv')
n <- 40000
length = rep(0, n)
for (k in 1:n){
length[k]=nchar(as.character(dat[[4]][k]))
}
summary(length)
hist(length, breaks = 50)
v = rep(' ', n)
l = rep(' ', n)
m = rep(0, n)
# extract the tweet content
for(k in 1:n){
v[k]=as.character(dat[[4]][k])
}
# separate the content by words
for(k in 1:n){
l[k] = strsplit(v[k], ' ')
}
# count the number of mentions
for(k in 1:n){
v=unlist(l[k])
for (i in 1:length(v)){
h = str_sub(v[i], 1, 1)
if (h[1] == '@'){
m[k] = m[k]+1
}
}
}
table(m)
summary(m)
hist(m)
dat$mention_count <- m
emp = ' '
for(t in 1:40000){
if (g[t]==10){
emp=paste(emp, k[t])
}
}
emp = ' '
for(t in 1:40000){
if (m[t]==10){
emp=paste(emp, k[t])
}
}
emp
emp = ' '
for(t in 1:40000){
if (m[t]==10){
emp=paste(emp, k[t])
}
}
emp
dat$mention_count <- m
dat$content[dat$mention_count == '10']
g1=rep(0,40000)
for(t in 1:40000){
v1=unlist(m[t])
for (f1 in 1:length(v1)){
h1 = str_sub(v1[f1], 1, 1)
if (h1[1] == '#'){
g1[t] = g1[t]+1
}
}
}
summary(g1)
hist(g1)
mean(g1)
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
Mode(g1)
g1=rep(0,40000)
for(t in 1:40000){
v1=unlist(l[t])
for (f1 in 1:length(v1)){
h1 = str_sub(v1[f1], 1, 1)
if (h1[1] == '#'){
g1[t] = g1[t]+1
}
}
}
summary(g1)
hist(g1)
mean(g1)
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
Mode(g1)
hist(g1)
mean(g1)
mean(g1)
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
Mode(g1)
g1=rep(0,40000)
for(t in 1:40000){
v1=unlist(l[t])
for (f1 in 1:length(v1)){
h1 = str_sub(v1[f1], 1, 1)
if (h1[1] == '#'){
g1[t] = g1[t]+1
}
}
}
summary(g1)
table(gi)
table(g1)
summary(g1)
table(g1)
hist(g1)
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
Mode(g1)
mean(g1)
g1=rep(0,40000)
for(k in 1:40000){
v1=unlist(l[k])
for (f1 in 1:length(v1)){
h1 = str_sub(v1[f1], 1, 1)
if (h1[1] == '#'){
g1[k] = g1[k]+1
}
}
}
summary(g1)
table(g1)
hist(g1)
d=rep(0,40000)
for(k in 1:40000){
v1=unlist(l[k])
for (f1 in 1:length(v1)){
h1 = str_sub(v1[f1], 1, 1)
if (h1[1] == '#'){
d[k] = d[k]+1
}
}
}
summary(d)
table(d)
hist(d)
mean(d)
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
Mode(d)
d=rep(0,40000)
for(k in 1:40000){
u=unlist(l[k])
for (f1 in 1:length(u)){
h1 = str_sub(u[f1], 1, 1)
if (u[1] == '#'){
d[k] = d[k]+1
}
}
}
summary(d)
summary(d)
d=rep(0,40000)
for(k in 1:40000){
v1=unlist(l[k])
for (f1 in 1:length(v1)){
h1 = str_sub(v1[f1], 1, 1)
if (h1[1] == '#'){
d[k] = d[k]+1
}
}
}
summary(d)
table(d)
d
v1
l
l[1]
setwd("/Users/liushaoyu/Documents/hw-stat133-liushaoyusz/hw04")
#3.3
# cannot contain spaces or punctuation symbols, and cannot start with or use only numbers.
#Count the number of hashtags in the tweet contents.
count_hash <- function(paulo){
return(str_count(paulo, pattern = "#[A-z_][\\w]+" ))
}
sum(count_hash(tweet$content)) #the number of hashtags in the tweet contents
#Count the number of hashtags in the tweet contents.
count_hash <- function(paulo){
return(str_count(paulo, pattern = "#[A-z_][\\w]+" ))
}
sum(count_hash(tweet$content)) #the number of hashtags in the tweet contents
View(dat)
#Count the number of hashtags in the tweet contents.
count_hash <- function(paulo){
return(str_count(paulo, pattern = "#[A-z_][\\w]+" ))
}
sum(count_hash(dat$content)) #the number of hashtags in the tweet contents
table(count_hash(dat$content))
barplot(table(count_hash(dat$content)))
sex <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
sex <- sex[!is.na(sex)]
#average length
mean(nchar(sex)-1) #7.775068
tab_hash <- table(nchar(sex)-1)
names(tab_hash)[which(tab_hash==max(tab_hash))]
#most common length is 4
#39261
#Count the number of hashtags in the tweet contents.
hashtags <- function(paulo){
return(str_count(paulo, pattern = "#[A-z_][\\w]+" ))
}
sum(count_hash(dat$content)) #the number of hashtags in the tweet contents
table(count_hash(dat$content))
barplot(table(count_hash(dat$content)))
sex <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
sex <- sex[!is.na(sex)]
#average length
mean(nchar(sex)-1) #7.775068
tab_hash <- table(nchar(sex)-1)
names(tab_hash)[which(tab_hash==max(tab_hash))]
#most common length is 4
#39261
# Count the number of hashtags in the tweet contents.
hashtags <- function(paulo){
return(str_count(paulo, pattern = "#[A-z_][\\w]+" ))
}
sum(count_hash(dat$content))
table(count_hash(dat$content))
barplot(table(count_hash(dat$content)))
sex <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
sex <- sex[!is.na(sex)]
# average length
mean(nchar(sex)-1)
tab_hash <- table(nchar(sex)-1)
# most common length
names(tab_hash)[which(tab_hash==max(tab_hash))]
# Count the number of hashtags in the tweet contents.
hashtags <- function(x){
return(str_count(x, pattern = "#[A-z_][\\w]+" ))
}
sum(count_hash(dat$content))
table(count_hash(dat$content))
barplot(table(count_hash(dat$content)))
sex <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
sex <- sex[!is.na(sex)]
# the average length
mean(nchar(sex)-1)
tab_hash <- table(nchar(sex)-1)
# the most common length
names(tab_hash)[which(tab_hash==max(tab_hash))]
# Count the number of hashtags in the tweet contents.
hashtags <- function(x){
return(str_count(x, pattern = "#[A-z_][\\w]+" ))
}
sum(hashtags(dat$content))
table(hashtags(dat$content))
barplot(table(hashtags(dat$content)))
sex <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
sex <- sex[!is.na(sex)]
# the average length
mean(nchar(sex)-1)
tab_hash <- table(nchar(sex)-1)
# the most common length
names(tab_hash)[which(hashtags==max(tab_hash))]
# Count the number of hashtags in the tweet contents.
hashtags <- function(x){
return(str_count(x, pattern = "#[A-z_][\\w]+" ))
}
sum(hashtags(dat$content))
table(hashtags(dat$content))
barplot(table(hashtags(dat$content)))
sex <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
sex <- sex[!is.na(sex)]
# the average length
mean(nchar(sex)-1)
tab_hash <- table(nchar(sex)-1)
# the most common length
names(hashtags)[which(hashtags==max(tab_hash))]
# Count the number of hashtags in the tweet contents.
hashtags <- function(x){
return(str_count(x, pattern = "#[A-z_][\\w]+" ))
}
sum(hashtags(dat$content))
table(hashtags(dat$content))
barplot(table(hashtags(dat$content)))
sex <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
sex <- sex[!is.na(sex)]
# the average length
mean(nchar(sex)-1)
tab_hash <- table(nchar(sex)-1)
# the most common length
names(hashtags)[which(hashtags==max(hashtags))]
# Count the number of hashtags in the tweet contents.
hashtags <- function(x){
return(str_count(x, pattern = "#[A-z_][\\w]+" ))
}
sum(hashtags(dat$content))
table(hashtags(dat$content))
barplot(table(hashtags(dat$content)))
sex <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
sex <- sex[!is.na(sex)]
# the average length
mean(nchar(sex)-1)
tab_hash <- table(nchar(sex)-1)
# the most common length
names(tab_hash)[which(tab_hash==max(tab_hash))]
# Count the number of hashtags in the tweet contents.
hashtags <- function(x){
return(str_count(x, pattern = "#[A-z_][\\w]+" ))
}
sum(hashtags(dat$content))
table(hashtags(dat$content))
barplot(table(hashtags(dat$content)))
sex <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
sex <- sex[!is.na(sex)]
# the average length
mean(nchar(sex)-1)
hash_mode <- table(nchar(sex)-1)
# the most common length
names(hash_mode)[which(hash_mode==max(hash_mode))]
# Count the number of hashtags in the tweet contents.
hashtags <- function(x){
return(str_count(x, pattern = "#[A-z_][\\w]+" ))
}
sum(hashtags(dat$content))
table(hashtags(dat$content))
barplot(table(hashtags(dat$content)))
# the average length
mean(nchar(sex)-1)
# the most common length
hash_mode <- table(nchar(sex)-1)
names(hash_mode)[which(hash_mode==max(hash_mode))]
barplot(table(hashtags(dat$content)))
c <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
c <- c[!is.na(c)]
# the average length
mean(nchar(c)-1)
# the most common length
hash_mode <- table(nchar(c)-1)
names(hash_mode)[which(hash_mode==max(hash_mode))]
knitr::opts_chunk$set(echo = TRUE, fig.path = '../images')
library(XML)
library(stringr)
library(ggmap)
library(plotly)
library(ggplot2)
source("../code/archive-functions.R")
source("../code/regex-functions.R")
raw_data <- read_archive('stringr')
raw_data
knitr::opts_chunk$set(echo = TRUE, fig.path = '../images')
library(XML)
library(stringr)
library(ggmap)
library(plotly)
library(ggplot2)
source("../code/archive-functions.R")
source("../code/regex-functions.R")
raw_data <- read_archive('stringr')
raw_data
knitr::opts_chunk$set(echo = TRUE, fig.path = '../images')
library(XML)
library(stringr)
library(ggmap)
library(plotly)
library(ggplot2)
source("../code/archive-functions.R")
source("../code/regex-functions.R")
raw_data <- read_archive('stringr')
raw_data
clean_data <- clean_archive(raw_data)
clean_data
write.csv(clean_data,'../data/stringr_archive.csv')
plot_archive(clean_data)
raw_data <- read_archive('stringr')
clean_data <- clean_archive(raw_data)
plot_archive(clean_data)
stringr_raw <- read_archive('stringr')
stringr_clean <- clean_archive(stringr_raw)
write.csv(stringr_clean, '../data/stringr_archive.csv')
dplyr_raw <- read_archive('dplyr')
dplyr_clean <- clean_archive(dplyr_raw)
write.csv(dplyr_clean, '../data/dplyr_archive.csv')
ggplot2_raw <- read_archive('ggplot2')
ggplot2_clean <- clean_archive(ggplot2_raw)
write.csv(ggplot2_clean, '../data/ggplot2_archive.csv')
XML_raw <- read_archive('XML')
XML_clean <- clean_archive(XML_raw)
write.csv(XML_clean, '../data/XML_archive.csv')
knitr_raw <- read_archive('knitr')
knitr_clean <- clean_archive(knitr_raw)
write.csv(knitr_clean, '../data/knitr_archive.csv')
dplyr_dat <- time_in_yr(dplyr_clean)
ggplot2_dat <- time_in_yr(ggplot2_clean)
XML_dat <- time_in_yr(XML_clean)
knitr_dat <- time_in_yr(knitr_clean)
ggplot() + geom_step(data=dplyr_dat, aes(x=time, y=size, colour='pink')) + geom_point() + geom_step(data=ggplot2_dat, aes(x=time, y=size), colour='green') + geom_point() + geom_step(data=XML_dat, aes(x=time, y=size), colour='purple') + geom_point() + geom_step(data=knitr_dat, aes(x=time, y=size), colour='blue') + geom_point() + xlab('date') + ylab('size(Kilobytes)')
dat <- dplyr_dat %>%
merge(ggplot2_dat,all=TRUE) %>%
merge(XML_dat,all=TRUE) %>%
merge(knitr_dat,all=TRUE)
ggplot(data = dat, aes(x = time, y = size, color = name)) + geom_step() + facet_wrap(~name, scales = 'free')
split_chars('Go Bear!')
split_chars('Expecto Patronum')
vec <- split_chars('Go Bear!')
num_vowels(vec)
count_vowels('The quick brown fox jumps over the lazy dog')
count_vowels('THE QUICK BROWN FOR JUMPS OVER THE LAZY DOG')
reverse_chars('gattaca')
reverse_chars('Lumox Maxima')
reverse_words('sentence! this reverse')
reverse_words('string')
dat <- read.csv('../data/text-emotion.csv')
n <- 40000
length = rep(0, n)
for (k in 1:n){
length[k]=nchar(as.character(dat[[4]][k]))
}
summary(length)
hist(length, breaks = 50)
v = rep(' ', n)
l = rep(' ', n)
m = rep(0, n)
# extract the tweet content
for(k in 1:n){
v[k]=as.character(dat[[4]][k])
}
# separate the content by words
for(k in 1:n){
l[k] = strsplit(v[k], ' ')
}
# count the number of mentions
for(k in 1:n){
v=unlist(l[k])
for (i in 1:length(v)){
h = str_sub(v[i], 1, 1)
if (h[1] == '@'){
m[k] = m[k]+1
}
}
}
table(m)
summary(m)
hist(m)
dat$mention_count <- m
dat$content[dat$mention_count == '10']
# Count the number of hashtags in the tweet contents
d=rep(0,40000)
for(k in 1:40000){
v1=unlist(l[k])
for (f1 in 1:length(v1)){
h1 = str_sub(v1[f1], 1, 1)
if (h1[1] == '#'){
d[k] = d[k]+1
}
}
}
# summaries
summary(d)
# frequencies
table(d)
# barplot of counts
hist(d)
# Count the number of hashtags in the tweet contents.
hashtags <- function(x){
return(str_count(x, pattern = "#[A-z_][\\w]+" ))
}
sum(hashtags(dat$content))
table(hashtags(dat$content))
c <- str_extract(dat$content,pattern = "#[A-z][\\w]+")
c <- c[!is.na(c)]
# the average length
mean(nchar(c)-1)
# the most common length
hash_mode <- table(nchar(c)-1)
names(hash_mode)[which(hash_mode==max(hash_mode))]
